defaults:
  - model_checkpoint.yaml
  - model_summary.yaml
  - rich_progress_bar.yaml
  - speed_monitor.yaml
  - learning_rate_monitor.yaml
  - _self_

model_checkpoint:
  _target_: lightning.pytorch.callbacks.ModelCheckpoint
  dirpath: ${paths.output_dir}/checkpoints # directory to save the model file
  filename: "epoch_{epoch:03d}" # checkpoint filename
  monitor: "val/reward" # name of the logged metric which determines when model is improving
  mode: "max" # "max" means higher metric value is better, can be also "min"
  save_last: True # additionally always save an exact copy of the last checkpoint to a file last.ckpt
  auto_insert_metric_name: False # when True, the checkpoints filenames will contain the metric name
  save_top_k: 1 # save k best models (determined by above metric), set to -1 to save all checkpoints,

model_summary:
  _target_: lightning.pytorch.callbacks.RichModelSummary
  max_depth: 5 # the maximum depth of layer nesting that the summary will include. change to -1 to show all. 5 strikes a good balance between readability and completeness

rich_progress_bar:
  _target_: lightning.pytorch.callbacks.RichProgressBar

speed_monitor:
  _target_: rl4co.utils.callbacks.speed_monitor.SpeedMonitor
  intra_step_time: True
  inter_step_time: True
  epoch_time: True

learning_rate_monitor:
  _target_: lightning.pytorch.callbacks.LearningRateMonitor
  logging_interval: epoch






